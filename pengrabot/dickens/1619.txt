Hi all, My database needs to have a pretty massive update_or_create about four times a day. There are anywhere from 1,000 to 50,000 records that need to be created or updated. Update_or_create works acceptably, but I'd love to up the speed. Basically, I have a ton of entries in a CSV and the unique identifier is the loan number (although I never defined it as the PK in the database, I don't think). I was going through row by row using dataReader and adding to a dictionary all my loans. Then I check against all my loans in the database and if that loan number is not there, it gets added to the to_update list. Here's what I have so far: This code is either non-working or ends up timing out my MySQL database. Any better ideas on how to go about this and/or fix my code? Thank you! Edit: It occurred to me it might be really fast to just delete all the loans that need to be updated and bulk_create them? Edit Edit: I'm using MySQL if that matters. I would start by enabling query logging in MySQL to see what queries are actually being generated and then making sure that you have indexes on the appropriate columns. No sense in trying to out think your database, it will tell you exactly what's going on if you ask it nicely. Bulk updating 50k rows should be pretty snappy unless there is something really wonky going on. First thought: Check the queries. A long running query shouldn't cause the connection to time out, but it's worth checking to make sure you aren't running into lock contention or some issue that'll make the insert always run slow regardless of what you do in Python. Outside of that, I'd personally try to turn this into a generator friendly form, so if Django does any batching behind the scenes you can avoid compiling everything all at once and putting it in memory just to have it sit in a queue. It's probably not going to give you orders of magnitude benefit, but I know MySQL by default has a max query size that could pose a problem for a query that's trying to create 50,000 row in it, so I hope Django can break up a massive insert or update to avoid hitting the cap and if it does then it makes sense to take advantage of that upstream if possible. casting to_update to a set can't hurt and using iteritems instead of items is better on the memory (assuming python2) It occurred to me it might be really fast to just delete all the loans that need to be updated and bulk_create them? I tried this, then luckily read a post somewhere about how this can cause you to overflow on your pk row. If you're doing 500k delete/adds a day, within a year or two you'll have a major problem on your hands. Indeed! Never ever do this! Besides blowing through a 32 bit integer in short time it also kills the use of ForeignKeys, if the unique fields you match on is not the primary key (like OP's loan_number). This naive delete/create approach is also a common pattern when using ManyToMany relations in Django.. it is evilllll... don't do it unless you like to run production on a timebomb (yes this happened, and it is why you need code reviews because there is always someone who doesn't know and 32 bits are not as big as it seems). We use this bulk_create/bulk_update pattern very often. Get keys you have in db Process input data, split into two collections based on new and existing keys Run your bulk_create and bulk_update loops If it is not working then there is an implementation issue, not a logical flaw. Your code looks a bit.. unfocussed (continue? setattr? in-operator on a huge list? hmm.. ). It will work if you rewrite it cleanly and in-control. Keep in mind there is a gaping race-condition between the moment you grab the existing keys and your final bulk operations run. Doesn't matter if your script is the only thing adding new records and nobody else can remove records while it runs. Maybe look into a transaction, although it will us a lot of memory. The ultimate solution is an UPSERT query but it is not available for Django (at least last time I looked) Database rule #0: determine if it's db related. Check mysql resources/waits/blocks during update. You may see cpu/disk/memory bottlenecks. Check running query logs for waits/locks. A blocking process will stop the show without any resources being hit. A few things will really put the brakes on. poor table design. Too many columns will thrash your memory and disk io. Bad data types like choosing text for a bit or number. related objects. Relationships with other tables as well as table indexes will add extra steps to each update/insert. Its a trade off better performance after the update for worse during the update. lack of indexing. You have a natural key in the csv you're not using. If its a number, making it the pk of the table will help a LOT. Even if its text it'll help. This is assuming the natural key is unique to the whole dataset and it helps if it'll never change. If you have no indexes at all, just adding one on the natural key will improve things a lot. Don't add everything. That's basically making a new table with the natural key as the table pk... Only now you have two tables to update. index fragmentation will negate a lot of index benefits. After doing a lot of updates or deletes you can end up with a lot of empty space in memory pages as well as links to data on other pages of memory. This means more disk io to read the data cold into memory and then your memory has a lot of wasted space in the memory pages which will fill up your memory faster. There are queries to check fragmentation levels. Careful not to drop the indexes if you're live. You can likely defragment online but its slow and uses a lot of resources. stats need to be updated on the tables/indexes After the data distribution changes through a lot of small changes or through use or large imports. I'm not sure how often mysql handles this automatically, if at all. If the stats are out of date/wrong you can miss good execution plans or even create abysmal plans. I've seen a query go from hours to seconds just by updating the stats. Have you tuned your mysql or are you using default settings. Like postures its understood that it'll be used as a db on a desktop in a lot of cases. I'm not sure if mysql were on the desktop side of settings like postures does. With postgres you want to learn how to tune it to work as a high end application database. Postgres has very clear documents on doing this and there are tools that scan your hardware, ask a few questions and give you a new settings file tuned to have your database ask like a greedy jerk hogging memory and taking cpu priority. Locking may be an issue where other queries Will prevent you from getting your own lock. Is suggest row level locks for the table if you can. If you have table locking then something working on really old data can stop you from updating/inserting. Normalize chunks of data where it makes sense. This means more work in identifying different entities and balancing your work and dev work against poor performance. You can get by without normalization at first and the pain gets worse with more data and users. An easy example of normalization would be If every csv record has 10 fields detailing the state a customer lives in. Well you have 10mil customers and only 50 states with their data being repeated. So you create a States table and link the customer record to the states key. When you need to update states you're only updating a tiny table. In your main customer table you're saving a lot of space which inserts/selects/updates faster. Less disk footprint means faster io, less memory used. The downside to normalization is that its more work to import a csv like this. Also any query selecting state & customer info has to do a join. Normalization tends to make dbas happy and piss off developers. The golden rule is to keep normalizing until it hurts more than help a. The take it back a step. If you have the options to use compression in indexes and your cpu is chillin, turn on compression. Now I have more than a decade of experience with sql server and a fair amount with postgres/oracle. Not a whole lot with mysql. Excluding nosql databases, most of the concepts apply across the board at a higher level, but naming and details differ. It's mostly a big shell game of managing your io, memory, cpu and bandwidth. As a rule, you want every record to take up the smallest amount of space. Use compression (usually), the smallest data type that works and normalize your data. 49.0k Subscribers 108 Online