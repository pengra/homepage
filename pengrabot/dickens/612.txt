http://www.nltk.org/nltk_data/ Table of Contents Each corpus reader provides a variety of methods to read data from the corpus, depending on the format of the corpus.  For example, plaintext corpora support methods to read the corpus as raw text, a list of words, a list of sentences, or a list of paragraphs. Each of these reader methods may be given a single document's item name or a list of document item names.  When given a list of document item names, the reader methods will concatenate together the contents of the individual documents. If the reader methods are called without any arguments, they will typically load all documents in the corpus. Here are the first few words from each of NLTK's plaintext corpora: Similarly, the Indian Language POS-Tagged Corpus includes samples of Indian text annotated with part-of-speech tags: The CoNLL corpora also provide chunk structures, which are encoded as flat trees.  The CoNLL 2000 Corpus includes phrasal chunks; and the CoNLL 2002 Corpus includes named entity chunks. Note Warning SemCor is a subset of the Brown corpus tagged with WordNet senses and named entities. Both kinds of lexical items include multiword units, which are encoded as chunks (senses and part-of-speech tags pertain to the entire chunk). Reading the Penn Treebank (Wall Street Journal sample): Reading the Sinica Treebank: Reading the CoNLL 2007 Dependency Treebanks: NLTK also provides a corpus reader for the York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE); but the corpus itself is not included in the NLTK data package.  If you install it yourself, you can use NLTK to access it: If the YCOE corpus is not available, you will get an error message when you try to access it: The NLTK data package also includes a number of lexicons and word lists.  These are accessed just like text corpora.  The following examples illustrate the use of the wordlist corpora: The CMU Pronunciation Dictionary corpus contains pronounciation transcriptions for over 100,000 words.  It can be accessed as a list of entries (where each entry consists of a word, an identifier, and a transcription) or as a dictionary from words to lists of transcriptions.  Transcriptions are encoded as tuples of phoneme strings. Please see the separate WordNet howto. Please see the separate FrameNet howto. Please see the separate PropBank howto. Please see the separate SentiWordNet howto. This method has an optional argument that specifies a document or a list of documents, allowing us to map from (one or more) documents to (one or more) categories: In addition to mapping between categories and documents, these corpora permit direct access to their contents via the categories.  Instead of accessing a subset of a corpus by specifying one or more fileids, we can identify one or more categories, e.g.: Note that it is an error to specify both documents and categories. In the context of a text categorization system, we can easily test if the category assigned to a document is correct as follows: A list of sentences from various sources, especially reviews and articles. Each line contains one sentence; sentences were separated by using a sentence tokenizer. Comparative sentences have been annotated with their type, entities, features and keywords. A list of positive and negative opinion words or sentiment words for English. The OpinionLexiconCorpusReader also provides shortcuts to retrieve positive/negative words: These two datasets respectively contain annotated customer reviews of 5 and 9 products from amazon.com. It is also possible to reach the same information directly from the stream: We can compute stats for specific product features: A list of pros/cons sentences for determining context (aspect) dependent sentiment words, which are then applied to sentiment analysis of comparative sentences. The Brown Corpus, annotated with WordNet senses. The Senseval 2 corpus is a word sense disambiguation corpus.  Each item in the corpus corresponds to a single ambiguous word.  For each of these words, the corpus contains a list of instances, corresponding to occurrences of that word.  Each instance provides the word; a list of word senses that apply to the word occurrence; and the word's context. The following code looks at instances of the word 'interest', and displays their local context (2 words on each side) and word sense(s): The Sentence Polarity dataset contains 5331 positive and 5331 negative processed sentences. The Shakespeare corpus contains a set of Shakespeare plays, formatted as XML files.  These corpora are returned as ElementTree objects: The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences. The Toolbox corpus distributed with NLTK contains a sample lexicon and several sample texts from the Rotokas language.  The Toolbox corpus reader returns Toolbox files as XML ElementTree objects.  The following example loads the Rotokas dictionary, and figures out the distribution of part-of-speech tags for reduplicated words. This example displays some records from a Rotokas text: The NLTK data package includes a fragment of the TIMIT Acoustic-Phonetic Continuous Speech Corpus.  This corpus is broken down into small speech samples, each of which is available as a wave file, a phonetic transcription, and a tokenized word list. The corpus reader can combine the word segmentation information with the phonemes to produce a single tree structure: The start time and stop time of each phoneme, word, and sentence are also available: We can use these times to play selected pieces of a speech sample: The corpus reader can also be queried for information about the speaker and sentence identifier for a given speech sample: Twitter is well-known microblog service that allows public data to be collected via APIs. NLTK's twitter corpus currently contains a sample of 20k Tweets retrieved from the Twitter Streaming API. The RTE (Recognizing Textual Entailment) corpus was derived from the RTE1, RTE2 and RTE3 datasets (dev and test data), and consists of a list of XML-formatted 'text'/'hypothesis' pairs. In the gold standard test sets, each pair is labeled according to whether or not the text 'entails' the hypothesis; the entailment value is mapped to an integer 1 (True) or 0 (False). The VerbNet corpus is a lexicon that divides verbs into classes, based on their syntax-semantics linking behavior.  The basic elements in the lexicon are verb lemmas, such as 'abandon' and 'accept', and verb classes, which have identifiers such as 'remove-10.1' and 'admire-31.2-1'.  These class identifiers consist of a representative verb selected from the class, followed by a numerical identifier.  The list of verb lemmas, and the list of class identifiers, can be retrieved with the following methods: See the Verbnet documentation, or the Verbnet files, for information about the structure of this xml.  As an example, we can retrieve a list of thematic roles for a given Verbnet class: The NPS Chat Corpus, Release 1.0 consists of over 10,000 posts in age-specific chat rooms, which have been anonymized, POS-tagged and dialogue-act tagged. In addition to the above methods for accessing tagged text, we can navigate the XML structure directly, as follows: The Multext-East Corpus consists of POS-tagged versions of George Orwell's book 1984 in 12 languages: English, Czech, Hungarian, Macedonian, Slovenian, Serbian, Slovak, Romanian, Estonian, Farsi, Bulgarian and Polish. The corpus can be accessed using the usual methods for tagged corpora. The tagset can be transformed from the Multext-East specific MSD tags to the Universal tagset using the "tagset" parameter of all functions returning tagged parts of the corpus. To create a new corpus reader, you will first need to look up the signature for that corpus reader's constructor.  Different corpus readers have different constructor signatures, but most of the constructor signatures have the basic form: If you wish to read your own plaintext corpus, which is stored in the directory '/usr/share/some-corpus', then you can create a corpus reader for it with: At a high level, corpora can be divided into three basic types: However, many individual corpora blur the distinctions between these types.  For example, corpora that are primarily lexicons may include token data in the form of example sentences; and corpora that are primarily token corpora may be accompanied by one or more word lists or other lexical data sets. Because corpora vary so widely in their information content, we have decided that it would not be wise to use separate corpus reader base classes for different corpus types.  Instead, we simply try to make the corpus readers consistent wherever possible, but let them differ where the underlying data itself differs. This method is mainly useful as a helper method when defining corpus data access methods, since data access methods can usually be called with a string argument (to get a view for a specific file), with a list argument (to get a view for a specific list of files), or with no argument (to get a view for the whole corpus). By only loading items from the file on an as-needed basis, corpus views maintain both memory efficiency and responsiveness.  The memory efficiency of corpus readers is important because some corpora contain very large amounts of data, and storing the entire data set in memory could overwhelm many machines.  The responsiveness is important when experimenting with corpora in interactive sessions and in in-class demonstrations. In the future, we may add additional corpus views that act like other basic data structures, such as dictionaries. In order to add support for new corpus formats, it is necessary to define new corpus reader classes.  For many corpus formats, writing new corpus readers is relatively straight-forward.  In this section, we'll describe what's involved in creating a new corpus reader.  If you do create a new corpus reader, we encourage you to contribute it back to the NLTK project. If you decide to write a new corpus reader from scratch, then you should first decide which data access methods you want the reader to provide, and what their signatures should be.  You should look at existing corpus readers that process corpora with similar data contents, and try to be consistent with those corpus readers whenever possible. If your corpus reader does not implement any customization parameters, then you can often just inherit the base class's constructor. (This is usually more appropriate for lexicons than for token corpora.) The toknum/filepos mapping is created lazily: it is initially empty, but every time a new block is read, the block's initial token is added to the mapping.  (Thus, the toknum/filepos map has one entry per block.) You can create your own corpus view in one of two ways: The following helper functions are used to create and then delete testing corpora that are stored in temporary directories.  These testing corpora are used to make sure the readers work correctly. The plaintext corpus reader is used to access corpora that consist of unprocessed plaintext data.  It assumes that paragraph breaks are indicated by blank lines.  Sentences and words can be tokenized using the default tokenizers, or by custom tokenizers specified as parameters to the constructor. The directory containing the corpus is corpus.root: We can get a list of words, or the raw string: Check that reading individual documents works, and reading all documents at once works: We're done with the test corpus: Test the plaintext corpora that come with nltk: The Tagged Corpus reader can give us words, sentences, and paragraphs, each tagged or untagged.  All of the read methods can take one item (in which case they return the contents of that file) or a list of documents (in which case they concatenate the contents of those files). By default, they apply to all documents in the corpus. The Brown Corpus uses the tagged corpus reader: Make sure we're picking up the right number of elements: Selecting classids based on various selectors: vnclass() accepts filenames, long ids, and short ids: fileids() can be used to get files based on verbnet class ids: longid() and shortid() can be used to convert identifiers: Select some corpus files to play with: Check that concatenation works as intended. First, do some tests with fairly small slices.  These will all generate tuple values. Choose a list of indices, based on the length, that covers the important corner cases: Test slicing with explicit start & stop value: Test slicing with stop=None: Test slicing with start=None: Test slicing with start=stop=None: Next, we'll do some tests with much longer slices.  These will generate LazySubsequence objects. Choose a list of indices, based on the length, that covers the important corner cases: Test slicing with explicit start & stop value: Test slicing with stop=None: Test slicing with start=None: Test slicing with start=stop=None: If multiple iterators are created for the same corpus view, their iteration can be interleaved: svn 5276 fixed a bug in the comment-stripping behavior of parse_sexpr_block. svn 5277 fixed a bug in parse_sexpr_block, which would cause it to enter an infinite loop if a file ended mid-sexpr, or ended with a token that was not followed by whitespace.  A related bug caused an infinite loop if the corpus ended in an unmatched close paren -- this was fixed in svn 5279 svn 5624 & 5265 fixed a bug in ConcatenatedCorpusView, which caused it to return the wrong items when indexed starting at any index beyond the first file. svn 7227 fixed a bug in the qc corpus reader, which prevented access to its tuples() method